{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нам необходимо создать для магазина инструмент, позволяющий разделять комментарии на позитивные и негативные, а затем отправлять негативные на модерацию. Для решения задачи мы будем использовать набор данных с разметкой о токсичности правок."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.1. Загрузим библиотеки, необходимые для решения нашей задачи и файл с размеченными данными."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /opt/conda/lib/python3.9/site-packages (3.2.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.6)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (4.61.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.21.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy) (49.6.0.post20210108)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.25.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (2.4.4)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy) (8.0.17)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy) (3.0.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (0.7.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy) (1.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy) (2.1.1)\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "Collecting en-core-web-sm==3.2.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.9 MB 2.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from en-core-web-sm==3.2.0) (3.2.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.25.1)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.2)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (49.6.0.post20210108)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.8)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.1)\n",
      "Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.17)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.4)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.61.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.10)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.7)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.10.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.2)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.4.0 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.7.8)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.7)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/conda/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.3.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2022.6.15)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.26.6)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.0.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /opt/conda/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.9/site-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.1.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "import sys\n",
    "!{sys.executable} -m pip install spacy\n",
    "!{sys.executable} -m spacy download en\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier \n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.2. Посмотрим и обработаем текст для выполнения нашей задачи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data = pd.read_csv('/datasets/toxic_comments.csv', index_col=0)\n",
    "display(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 159292 entries, 0 to 159450\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    159292 non-null  object\n",
      " 1   toxic   159292 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#посмотрмим, есть ли пропуски\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим, есть ли дубликаты\n",
    "data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     object\n",
       "toxic     int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим тип данных\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.3.Обработаем текст с помощью регулярных выражений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub('\\W', ' ', text)\n",
    "    text = re.sub('\\s+', ' ', text)\n",
    "    text = text.strip(' ')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [00:09<00:00, 16444.77it/s]\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas()\n",
    "data['text'] = data['text'].progress_apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 159292/159292 [17:12<00:00, 154.22it/s]\n"
     ]
    }
   ],
   "source": [
    "#лемматизация текста\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
    "def spacy_lemm(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    return \" \".join([token.lemma_ for token in doc])\n",
    "\n",
    "data['text'] = data['text'].progress_apply(spacy_lemm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод:\n",
    "\n",
    "1. В нашем распоряжении 159291 строчки размеченных данных;\n",
    "2. дубликатов и пропусков нет;\n",
    "3. типы данных подходящие;\n",
    "4. с помощью регулярных выражений мы обработали текст для дальнейшего изучения;\n",
    "5. лемматизировали текст."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.1. Подготовим признаки и целевой признак для обучения, выделим выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('toxic', axis=1)\n",
    "y = data['toxic']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.2.Обучим модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А. LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__C': 10, 'clf__class_weight': 'balanced'}\n",
      "0.7715114676784594\n",
      "CPU times: user 14min 33s, sys: 9min 29s, total: 24min 3s\n",
      "Wall time: 24min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, stop_words=stopwords)),\n",
    "    ('clf', LogisticRegression(random_state=12345))])\n",
    "\n",
    "params = {'clf__C': [0.1, 1, 10, 100],\n",
    "          'clf__class_weight': ['balanced', None]}\n",
    "\n",
    "lr_grid = GridSearchCV(estimator=lr_pipe, param_grid=params, cv=3, scoring='f1', n_jobs=-1, refit=False)\n",
    "lr_grid.fit(X_train['text'], y_train)\n",
    "lr_best_paramms = lr_grid.best_params_\n",
    "\n",
    "print(lr_best_paramms)\n",
    "print(lr_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Б. LGBMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__learning_rate': 0.25, 'clf__max_depth': -1, 'clf__n_estimators': 200}\n",
      "0.7706727368140607\n",
      "CPU times: user 1h 25min 8s, sys: 27.9 s, total: 1h 25min 36s\n",
      "Wall time: 1h 25min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lgb_pipe = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(ngram_range=(1,3), min_df=3, max_df=0.9, use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, stop_words=stopwords)),\n",
    "    ('clf', LGBMClassifier(random_state=12345))])\n",
    "\n",
    "params = {\n",
    "  'clf__n_estimators': [200],\n",
    "  'clf__learning_rate': [0.15, 0.25],\n",
    "  'clf__max_depth': [8, 10, -1]}\n",
    "\n",
    "lgb_grid = GridSearchCV(estimator=lgb_pipe, param_grid=params, cv=3, scoring='f1', n_jobs=-1, refit=False)\n",
    "lgb_grid.fit(X_train['text'], y_train)\n",
    "lgb_best_params = lgb_grid.best_params_\n",
    "\n",
    "print(lgb_best_params)\n",
    "print(lgb_grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подобрали параметры с помощью GridSearchCV с расчетом TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize = TfidfVectorizer(ngram_range=(1,3),\n",
    "               min_df=3, max_df=0.9, use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorize.fit_transform(X_train['text'])\n",
    "X_test = vectorize.transform(X_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 34.7 s, sys: 33.9 s, total: 1min 8s\n",
      "Wall time: 1min 8s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight='balanced', random_state=12345)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lr_m = LogisticRegression(C=10, class_weight='balanced', random_state=12345)\n",
    "lr_m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 25s, sys: 2.29 s, total: 8min 27s\n",
      "Wall time: 8min 31s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(learning_rate=0.15, n_estimators=200, random_state=12345)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "lgb_m = LGBMClassifier(learning_rate=0.15, max_depth=-1, n_estimators=200, random_state = 12345)\n",
    "lgb_m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.3. Посчитаем F! нащих моделей на тестовой выборке для проверки эффективности и выбора лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(fitted_model):\n",
    "    test_pred = fitted_model.predict(X_test)\n",
    "    test_f1 = f1_score(y_test, test_pred)\n",
    "    \n",
    "    print('F1 on test: {:.3f}'.format(test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 on test: 0.777\n"
     ]
    }
   ],
   "source": [
    "scoring(lgb_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 on test: 0.788\n"
     ]
    }
   ],
   "source": [
    "scoring(lr_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Вывод:\n",
    "\n",
    "мы обучили модели LGBMClassifier и LogisticRegression, посчитали F! нащих моделей на тестовой выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для разработки  инструмента, позволяющего разделять комментарии на позитивные и негативные, а затем отправлять негативные на модерацию, мы рассмотрели и обработали данные с разметкой о токсичности правок.\n",
    "Затем из этих данных сделали выборки для обучения моделей, обучили модели LGBMClassifier и LogisticRegression, посчитали F! нащих моделей на тестовой выборке.\n",
    "\n",
    "F1 LGBMClassifier: 0.777\n",
    "F1 LogisticRegression: 0.788\n",
    "\n",
    "Обе модели соответствуют условию задачи (F1 не меньше 0.75), но F1 LogisticRegression больше, при этом время обучения модели значительно меньше.\n",
    "Соответственно на основании проведенных вычислений предложим заказчику лучшую модель, а именно: LogisticRegression."
   ]
  }
 ],
 "metadata": {
  "ExecuteTimeLog": [
   {
    "duration": 2424,
    "start_time": "2023-05-29T10:47:48.008Z"
   },
   {
    "duration": 4071,
    "start_time": "2023-05-29T10:49:06.695Z"
   },
   {
    "duration": 946,
    "start_time": "2023-05-29T10:49:31.217Z"
   },
   {
    "duration": 38,
    "start_time": "2023-05-29T10:49:48.515Z"
   },
   {
    "duration": 116,
    "start_time": "2023-05-29T10:50:35.485Z"
   },
   {
    "duration": 33,
    "start_time": "2023-05-29T10:50:45.096Z"
   },
   {
    "duration": 262,
    "start_time": "2023-05-29T10:51:49.692Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-29T10:52:49.402Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-29T10:54:21.734Z"
   },
   {
    "duration": 7695,
    "start_time": "2023-05-29T10:54:47.502Z"
   },
   {
    "duration": 11,
    "start_time": "2023-05-29T11:00:27.511Z"
   },
   {
    "duration": 37,
    "start_time": "2023-05-29T11:00:39.911Z"
   },
   {
    "duration": 1332750,
    "start_time": "2023-05-29T11:09:27.500Z"
   },
   {
    "duration": 8567072,
    "start_time": "2023-05-29T11:31:40.252Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-29T13:54:27.326Z"
   },
   {
    "duration": 25829,
    "start_time": "2023-05-29T13:54:27.338Z"
   },
   {
    "duration": 63214,
    "start_time": "2023-05-29T13:55:58.032Z"
   },
   {
    "duration": 1017716,
    "start_time": "2023-05-29T14:15:21.068Z"
   },
   {
    "duration": 3,
    "start_time": "2023-05-29T14:32:18.785Z"
   },
   {
    "duration": 7999,
    "start_time": "2023-05-29T14:32:18.790Z"
   },
   {
    "duration": 57,
    "start_time": "2023-05-29T14:32:26.791Z"
   },
   {
    "duration": 73,
    "start_time": "2023-05-30T03:57:00.938Z"
   },
   {
    "duration": 1757,
    "start_time": "2023-05-30T03:57:06.391Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-30T03:57:15.597Z"
   },
   {
    "duration": 3832,
    "start_time": "2023-05-30T03:57:17.502Z"
   },
   {
    "duration": 19729,
    "start_time": "2023-05-30T04:01:00.961Z"
   },
   {
    "duration": 1085,
    "start_time": "2023-05-30T04:03:03.922Z"
   },
   {
    "duration": 509,
    "start_time": "2023-05-30T04:07:25.958Z"
   },
   {
    "duration": 551,
    "start_time": "2023-05-30T04:08:01.509Z"
   },
   {
    "duration": 129,
    "start_time": "2023-05-30T04:08:39.301Z"
   },
   {
    "duration": 166,
    "start_time": "2023-05-30T04:08:49.230Z"
   },
   {
    "duration": 135,
    "start_time": "2023-05-30T04:08:54.196Z"
   },
   {
    "duration": 4,
    "start_time": "2023-05-30T04:10:44.831Z"
   },
   {
    "duration": 20,
    "start_time": "2023-05-30T04:11:19.358Z"
   },
   {
    "duration": 11686,
    "start_time": "2023-05-30T04:11:52.632Z"
   },
   {
    "duration": 9377,
    "start_time": "2023-05-30T04:12:04.320Z"
   },
   {
    "duration": 1102559,
    "start_time": "2023-05-30T04:13:31.970Z"
   },
   {
    "duration": 15725,
    "start_time": "2023-05-30T04:39:54.345Z"
   },
   {
    "duration": 957,
    "start_time": "2023-05-30T04:40:10.072Z"
   },
   {
    "duration": 29,
    "start_time": "2023-05-30T04:40:11.030Z"
   },
   {
    "duration": 46,
    "start_time": "2023-05-30T04:40:11.061Z"
   },
   {
    "duration": 237,
    "start_time": "2023-05-30T04:40:11.112Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-30T04:40:11.351Z"
   },
   {
    "duration": 23,
    "start_time": "2023-05-30T04:40:11.359Z"
   },
   {
    "duration": 10370,
    "start_time": "2023-05-30T04:40:11.391Z"
   },
   {
    "duration": 1122561,
    "start_time": "2023-05-30T04:40:21.764Z"
   },
   {
    "duration": 36,
    "start_time": "2023-05-30T04:59:04.328Z"
   },
   {
    "duration": 1543497,
    "start_time": "2023-05-30T04:59:04.366Z"
   },
   {
    "duration": 200,
    "start_time": "2023-05-30T10:47:50.020Z"
   },
   {
    "duration": 15900,
    "start_time": "2023-05-30T10:48:13.583Z"
   },
   {
    "duration": 3680,
    "start_time": "2023-05-30T10:48:29.485Z"
   },
   {
    "duration": 31,
    "start_time": "2023-05-30T10:48:33.167Z"
   },
   {
    "duration": 32,
    "start_time": "2023-05-30T10:48:33.199Z"
   },
   {
    "duration": 224,
    "start_time": "2023-05-30T10:48:33.239Z"
   },
   {
    "duration": 6,
    "start_time": "2023-05-30T10:48:33.464Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-30T10:48:33.472Z"
   },
   {
    "duration": 9724,
    "start_time": "2023-05-30T10:48:33.479Z"
   },
   {
    "duration": 1033578,
    "start_time": "2023-05-30T10:48:43.204Z"
   },
   {
    "duration": 45,
    "start_time": "2023-05-30T11:05:56.784Z"
   },
   {
    "duration": 1445550,
    "start_time": "2023-05-30T11:05:56.832Z"
   },
   {
    "duration": 5153349,
    "start_time": "2023-05-30T11:30:02.384Z"
   },
   {
    "duration": 8,
    "start_time": "2023-05-30T12:55:55.735Z"
   },
   {
    "duration": 26926,
    "start_time": "2023-05-30T12:55:55.745Z"
   },
   {
    "duration": 68688,
    "start_time": "2023-05-30T12:56:22.672Z"
   },
   {
    "duration": 511235,
    "start_time": "2023-05-30T12:57:31.362Z"
   },
   {
    "duration": 5,
    "start_time": "2023-05-30T13:06:02.600Z"
   },
   {
    "duration": 8679,
    "start_time": "2023-05-30T13:06:02.607Z"
   },
   {
    "duration": 78,
    "start_time": "2023-05-30T13:06:11.288Z"
   }
  ],
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
